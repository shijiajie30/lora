"""
æ¨¡å‹å¯¹æ¯”å·¥å…· - å¯¹æ¯”å¾®è°ƒå‰åçš„æ¨¡å‹å›ç­”
"""

import torch
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import platform


def parse_args():
    parser = argparse.ArgumentParser(description="å¯¹æ¯”å¾®è°ƒå‰åçš„æ¨¡å‹å›ç­”")
    parser.add_argument("--base_model", type=str, 
                        default="./models/LLM-Research/Llama-3.2-3B-Instruct",
                        help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_weights", type=str, required=True,
                        help="LoRAæƒé‡è·¯å¾„ï¼ˆå¾®è°ƒåçš„æ¨¡å‹ï¼‰")
    parser.add_argument("--prompt", type=str, default=None,
                        help="æµ‹è¯•é—®é¢˜")
    parser.add_argument("--test_file", type=str, default=None,
                        help="æµ‹è¯•é—®é¢˜æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼‰")
    parser.add_argument("--max_new_tokens", type=int, default=512,
                        help="ç”Ÿæˆçš„æœ€å¤§tokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7,
                        help="é‡‡æ ·æ¸©åº¦")
    return parser.parse_args()


def format_prompt(text):
    """æ ¼å¼åŒ–ä¸ºLlamaæŒ‡ä»¤æ ¼å¼"""
    return f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"


def load_models(base_model_path, lora_weights_path):
    """åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹"""
    
    print("=" * 80)
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    print("=" * 80)
    
    # æ£€æµ‹è®¾å¤‡
    is_macos = platform.system() == "Darwin"
    if torch.cuda.is_available():
        device = "cuda"
        dtype = torch.bfloat16
        print("âœ“ ä½¿ç”¨ CUDA GPU")
    elif torch.backends.mps.is_available() and is_macos:
        device = "mps"
        dtype = torch.float32
        print("âœ“ ä½¿ç”¨ Apple Silicon (MPS)")
    else:
        device = "cpu"
        dtype = torch.float32
        print("âœ“ ä½¿ç”¨ CPU")
    
    # åŠ è½½tokenizer
    print(f"\nåŠ è½½ tokenizer: {base_model_path}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½åŸå§‹æ¨¡å‹ï¼ˆæœªå¾®è°ƒï¼‰
    print(f"\n[1/2] åŠ è½½åŸå§‹æ¨¡å‹ï¼ˆæœªå¾®è°ƒï¼‰...")
    base_model_original = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=dtype,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True,
    )
    if device != "cuda":
        base_model_original = base_model_original.to(device)
    base_model_original.eval()
    print("âœ“ åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½å¾®è°ƒæ¨¡å‹
    print(f"\n[2/2] åŠ è½½å¾®è°ƒæ¨¡å‹ï¼ˆå¸¦LoRAï¼‰...")
    print(f"LoRAæƒé‡: {lora_weights_path}")
    base_model_finetuned = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=dtype,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True,
    )
    if device != "cuda":
        base_model_finetuned = base_model_finetuned.to(device)
    
    # åŠ è½½LoRAæƒé‡å¹¶åˆå¹¶
    finetuned_model = PeftModel.from_pretrained(base_model_finetuned, lora_weights_path)
    finetuned_model = finetuned_model.merge_and_unload()
    finetuned_model.eval()
    print("âœ“ å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
    
    print("\n" + "=" * 80)
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("=" * 80 + "\n")
    
    return base_model_original, finetuned_model, tokenizer, device


def generate_response(model, tokenizer, prompt, args, device):
    """ç”Ÿæˆå›ç­”"""
    
    formatted_prompt = format_prompt(prompt)
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=0.9,
            top_k=50,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # æå–assistantçš„å›å¤
    assistant_marker = "<|start_header_id|>assistant<|end_header_id|>\n\n"
    if assistant_marker in full_response:
        response = full_response.split(assistant_marker)[-1]
        response = response.replace("<|eot_id|>", "").replace("<|end_of_text|>", "").strip()
    else:
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if prompt in response:
            response = response.split(prompt)[-1].strip()
    
    return response


def compare_responses(original_model, finetuned_model, tokenizer, prompt, args, device):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”"""
    
    print("\n" + "=" * 80)
    print("é—®é¢˜:")
    print("-" * 80)
    print(prompt)
    print("=" * 80)
    
    # åŸå§‹æ¨¡å‹å›ç­”
    print("\n[åŸå§‹æ¨¡å‹] ç”Ÿæˆä¸­...")
    original_response = generate_response(original_model, tokenizer, prompt, args, device)
    
    # å¾®è°ƒæ¨¡å‹å›ç­”
    print("[å¾®è°ƒæ¨¡å‹] ç”Ÿæˆä¸­...\n")
    finetuned_response = generate_response(finetuned_model, tokenizer, prompt, args, device)
    
    # æ˜¾ç¤ºå¯¹æ¯”
    print("=" * 80)
    print("ğŸ“Š å›ç­”å¯¹æ¯”")
    print("=" * 80)
    
    print("\nğŸ”µ åŸå§‹æ¨¡å‹å›ç­”ï¼ˆæœªå¾®è°ƒï¼‰:")
    print("-" * 80)
    print(original_response)
    
    print("\nğŸŸ¢ å¾®è°ƒæ¨¡å‹å›ç­”ï¼ˆLoRAå¾®è°ƒåï¼‰:")
    print("-" * 80)
    print(finetuned_response)
    
    print("\n" + "=" * 80)
    
    # ç®€å•çš„ç»Ÿè®¡
    print("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  åŸå§‹å›ç­”é•¿åº¦: {len(original_response)} å­—ç¬¦")
    print(f"  å¾®è°ƒå›ç­”é•¿åº¦: {len(finetuned_response)} å­—ç¬¦")
    print("=" * 80 + "\n")


def main():
    args = parse_args()
    
    # åŠ è½½æ¨¡å‹
    original_model, finetuned_model, tokenizer, device = load_models(
        args.base_model, args.lora_weights
    )
    
    # æµ‹è¯•é—®é¢˜
    test_questions = []
    
    if args.test_file:
        # ä»æ–‡ä»¶è¯»å–æµ‹è¯•é—®é¢˜
        with open(args.test_file, 'r', encoding='utf-8') as f:
            test_questions = [line.strip() for line in f if line.strip()]
        print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(test_questions)} ä¸ªæµ‹è¯•é—®é¢˜\n")
    elif args.prompt:
        test_questions = [args.prompt]
    else:
        # é»˜è®¤æµ‹è¯•é—®é¢˜
        test_questions = [
            "ä»€ä¹ˆæ˜¯åˆåŒæ³•ï¼Ÿ",
            "åŠ³åŠ¨åˆåŒçº çº·å¦‚ä½•å¤„ç†ï¼Ÿ",
            "å¦‚ä½•èµ·è¯‰æ¬ æ¬¾äººï¼Ÿ"
        ]
        print("ä½¿ç”¨é»˜è®¤æµ‹è¯•é—®é¢˜\n")
    
    # å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œå¯¹æ¯”
    for i, question in enumerate(test_questions, 1):
        if len(test_questions) > 1:
            print(f"\n{'='*80}")
            print(f"æµ‹è¯• {i}/{len(test_questions)}")
            print(f"{'='*80}")
        
        compare_responses(
            original_model, 
            finetuned_model, 
            tokenizer, 
            question, 
            args, 
            device
        )
        
        if i < len(test_questions):
            input("\næŒ‰ Enter ç»§ç»­ä¸‹ä¸€ä¸ªé—®é¢˜...")
    
    print("\nâœ“ å¯¹æ¯”å®Œæˆï¼")


if __name__ == "__main__":
    main()

